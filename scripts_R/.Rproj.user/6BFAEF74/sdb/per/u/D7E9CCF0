{
    "contents" : "library(ngram)\n\nstr <- 'hello worlds I am a goat'\nngram_asweka(str, min = 2, max = 2, sep = \" \")\n\n\n\n\nrm(list = ls())\nwd <- 'C:/Users/KLN/some_r'\nsetwd(wd)\n\nlibrary(tm)\n\n## building a document-term matrix \n\nload('kjv.RData')\n# word frequencies\nbooks.dtm <- DocumentTermMatrix(books.cor, control = list(minWordLength = 2))\n\n\n\nbooks.dtm <- DocumentTermMatrix(books.cor, control=list(tokenize = ngram))\n\n\nlibrary(RWeka)\nhelp(package = 'RWeka')\nweka.dtm <- DocumentTermMatrix(books.cor, control=list(tokenize = NGramTokenizer))\n\n?NGramTokenizer\n\n###\ndd = \"C:/Users/KLN/some_r/data/kjv_books\";\nsetwd(dd)\n\n# import plain text files in the directory dd containing Latin (lat) texts\nbooks.cor  <- Corpus(DirSource(dd, encoding = \"UTF-8\"), readerControl = list(language = \"lat\"))\nnames(books.cor) <- gsub(\"\\\\..*\",\"\",names(books.cor))# remove ending\nfilenames <- names(books.cor)\n\n# preprocess\nbooks.cor <- tm_map(books.cor, PlainTextDocument)\nbooks.cor <- tm_map(books.cor, content_transformer(tolower))\nbooks.cor <- tm_map(books.cor, removePunctuation)\nbooks.cor <- tm_map(books.cor, removeNumbers)\n#books.cor <- tm_map(books.cor, removeWords, stopwords(\"english\"))\nbooks.cor <- tm_map(books.cor, stemDocument)\nbooks.cor <- tm_map(books.cor, stripWhitespace)\n\n\nweka.dtm <- DocumentTermMatrix(books.cor, control=list(tokenize = NGramTokenizer))\n\nngram.dtm <- DocumentTermMatrix(books.cor, control=list(tokenize = ngram))\n?ngram\n\n\n\n\n\nlibrary(NLP)\n?ngram\n\nBigramTokenizer <-\n  function(x)\n    unlist(lapply(ngrams(words(x), 2), paste, collapse = \" \"), use.names = FALSE)\n\ntdm <- DocumentTermMatrix(books.cor, control = list(tokenize = BigramTokenizer))\n\n\n############\nlibrary(tm); library(tau);\n\ntokenize_ngrams <- function(x, n=3) return(rownames(as.data.frame(unclass(textcnt(x,method=\"string\",n=n)))))\n\ntexts <- c(\"This is the first document.\", \"This is the second file.\", \"This is the third text.\")\ncorpus <- Corpus(VectorSource(texts))\nmatrix.m <- DocumentTermMatrix(corpus,control=list(tokenize=tokenize_ngrams))\n\n###\nlibrary(RTextTools)\ntexts <- c(\"This is the first document.\", \"This is the second file.\", \"This is the third text.\")\nmatrix <- create_matrix(texts,ngramLength=3)\n\n\n###\n\n\n#User Defined Functions\nTrim <- function (x) gsub(\"^\\\\s+|\\\\s+$\", \"\", x)\n\nbreaker <- function(x) unlist(strsplit(x, \"[[:space:]]|(?=[.!?*-])\", perl=TRUE))\n\nstrip <- function(x, digit.remove = TRUE, apostrophe.remove = FALSE){\n  strp <- function(x, digit.remove, apostrophe.remove){\n    x2 <- Trim(tolower(gsub(\".*?($|'|[^[:punct:]]).*?\", \"\\\\1\", as.character(x))))\n    x2 <- if(apostrophe.remove) gsub(\"'\", \"\", x2) else x2\n    ifelse(digit.remove==TRUE, gsub(\"[[:digit:]]\", \"\", x2), x2)\n  }\n  unlist(lapply(x, function(x) Trim(strp(x =x, digit.remove = digit.remove, \n                                         apostrophe.remove = apostrophe.remove)) ))\n}\n\nunblanker <- function(x)subset(x, nchar(x)>0)\n\n#Fake Text Data\nx <- \"I like green eggs and ham.  They are delicious.  They taste so yummy.  I'm talking about ham and eggs of course\"\n\n#The code using Base R to Do what you want\nbreaker(x)\nstrip(x)\nwords <- unblanker(breaker(strip(x)))\ntextDF <- as.data.frame(table(words))\ntextDF$characters <- sapply(as.character(textDF$words), nchar)\ntextDF2 <- textDF[order(-textDF$characters, textDF$Freq), ]\nrownames(textDF2) <- 1:nrow(textDF2)\ntextDF2\nsubset(textDF2, characters%in%2:3)\n\n####\n\n\nbigramTokenizer <- function(x) {\n  x <- as.character(x)\n  \n  # Find words\n  one.list <- c()\n  tryCatch({\n    one.gram <- ngram::ngram(x, n = 1)\n    one.list <- ngram::get.ngrams(one.gram)\n  }, \n  error = function(cond) { warning(cond) })\n  \n  # Find 2-grams\n  two.list <- c()\n  tryCatch({\n    two.gram <- ngram::ngram(x, n = 2)\n    two.list <- ngram::get.ngrams(two.gram)\n  },\n  error = function(cond) { warning(cond) })\n  \n  res <- unlist(c(one.list, two.list))\n  res[res != '']\n}\n\n# test\ndtmTest <- lapply(myCorpus.3, bigramTokenizer)\n# run\ndtm <- DocumentTermMatrix(myCorpus.3, control = list(tokenize = bigramTokenizer))\n\n\n####\n\n\n\n\n\n\n\n\n\n",
    "created" : 1469622733069.000,
    "dirty" : true,
    "encoding" : "",
    "folds" : "",
    "hash" : "1070190756",
    "id" : "D7E9CCF0",
    "lastKnownWriteTime" : 7011605692497750387,
    "path" : null,
    "project_path" : null,
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 21,
    "source_on_save" : false,
    "type" : "r_source"
}