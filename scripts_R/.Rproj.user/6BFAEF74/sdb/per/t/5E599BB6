{
    "contents" : "# word associations\n\nrm(list = ls())\nwd <- 'C:/Users/KLN/some_r'\nsetwd(wd)\nsource('util_fun.R')\nload('kjv.RData')\n\n# recursive import text function\ninput.dir <- \"data/kjv_books\"\nfiles.v <- dir(input.dir,\"\\\\.txt$\")\nmaketext.l <- function(files.v, input.dir){\n  text.word.l <- list() # set up empty list\n  for(i in 1:length(files.v)){ # loop over the files.v in input.dir\n    text.v <- scan(paste(input.dir, files.v[i], sep=\"/\"), what=\"character\", sep=\"\\n\") # read a file\n    text.v <- paste(text.v, collapse=\" \") # collapse lines\n    text.lower.v <- tolower(text.v) # casefolding\n    text.words.v <- strsplit(text.lower.v, \"\\\\W\") # tokenize\n    text.words.v <- unlist(text.words.v) # transform list to vector\n    text.words.v <- text.words.v[which(text.words.v!=\"\")] # remove blanks\n    text.word.l[[files.v[i]]] <- text.words.v # update list\n    # add more preprocessing steps\n  }\n  return(text.word.l)\n}\ntext.l <- maketext.l(files.v,input.dir)\nnames(text.l) <- gsub(\"\\\\..*\",\"\",names(text.l))\n\n# function for displaying file names of list corpus\nshowfiles <- function(filename.v){\n  for(i in 1:length(filename.v)){\n    cat(i, filename.v[i], \"\\n\", sep=\" \")# concatenate and print\n  }\n}\n\n\n\n# keyword in context search\nkwicsearch <- function(data.l){\n  showfiles(names(data.l))\n  fileid <- as.numeric(readline('What file would you like to examine? Enter file number: \\n'))\n  context <- as.numeric(readline('How large a span would you like? Enter a number for LHS & RHS: \\n'))# left/right hand side\n  keyword <- tolower(readline('Enter a keyword: \\n'))\n  hits.v <- which(data.l[[fileid]] == keyword)\n  output.l <- list()\n  if(length(hits.v > 0)){\n    for(h in 1:length(hits.v)){\n      start <- hits.v[h] - context\n      if(start < 1){\n        start <- 1\n      }\n      end <- hits.v[h]+context\n      cat(data.l[[fileid]][start:end], '\\n')\n      output.l[[h]] <- paste(data.l[[fileid]][start:end], collapse = ' ')\n    }\n    \n  }\n  return(output.l)\n}\n\nkwicsearch(text.l)\n\n### one collocation on a node in text\n# get a text from the list corpus\natext.v <- paste(text.l[[which(names(text.l) == 'Matthew')]], collapse = ' ')\n# a bit more quick and dirty preprocessing\nlibrary(tm)\natext.cor <- Corpus(VectorSource(atext.v))\natext.cor <- tm_map(atext.cor, removeNumbers)\n#atext.cor <- tm_map(atext.cor, removeWords, stopwords('english'))\n#atext.cor <- tm_map(atext.cor, stemDocument)\n\n# use n-gram tokenizer\nlibrary(RWeka)\nspan <- 5 # word span on either side (LHS/RHS)\nspan1 <- 1+span*2 # window size\nngramtokenizer <- function(x) NGramTokenizer(x, Weka_control(min = span1, max = span1))\ntdm <- TermDocumentMatrix(atext.cor, control = list(tokenize = ngramtokenizer))\ninspect(tdm)\n\n# set node word and find ngrams with node\nword <- 'hate' \nnodengrams <- tdm$dimnames$Terms[grep(word, tdm$dimnames$Terms)]\n\n# sort out ngrams that does not have the word in the middle (remove dublicated)\nnodengrams <- nodengrams[sapply(nodengrams, function(i) {\n  tmp <- unlist(strsplit(i, split=\" \"))# tokenize\n  tmp <- tmp[length(tmp) - span]# middle word\n  tmp} == word)]# is middle the node\nprint(nodengrams)\n# find collocate\nword1 <- 'love'\nnodengrams2 <- nodengrams[grep(word1, nodengrams)]\nprint(nodengrams2)\n# number of collocations\nlength(nodengrams2)\n\n# calculate pointwise mutual information\nA <- length(which(text.l[[which(names(text.l) == 'Matthew')]] == word))\nB <- length(which(text.l[[which(names(text.l) == 'Matthew')]] == word1))\nAB <- length(nodengrams2)\nN <- length(text.l[[which(names(text.l) == 'Matthew')]])\n# Church and Hanks' association ratio\nMI <- log2((AB/N)/(A/N*B/N))\n# ~ \nMI <- log2((AB*N)/(A*B))\n\n\n##### a slightly different take on associations that scale with tm\nrm(list = ls())\nlibrary(tm)\ndd = \"C:/Users/KLN/some_r/data/kjv_books\";\nbooks.cor  <- Corpus(DirSource(dd, encoding = \"UTF-8\"), readerControl = list(language = \"lat\"))\nnames(books.cor) <- gsub(\"\\\\..*\",\"\",names(books.cor))# remove ending\nfilenames <- names(books.cor)\nbooks.cor <- tm_map(books.cor, PlainTextDocument)\nbooks.cor <- tm_map(books.cor, content_transformer(tolower))\nbooks.cor <- tm_map(books.cor, removePunctuation)\nbooks.cor <- tm_map(books.cor, removeNumbers)\nbooks.cor <- tm_map(books.cor, removeWords, stopwords(\"english\"))\n#books.cor <- tm_map(books.cor, stemDocument)\nbooks.cor <- tm_map(books.cor, stripWhitespace)\nbooks.dtm <- DocumentTermMatrix(books.cor)\nbooks.dtm$dimnames$Docs <- filenames\n\nl.v <- as.vector(inspect(books.dtm[,'love']))\nh.v <- as.vector(inspect(books.dtm[,'hate']))\ncor(l.v,h.v)\n###\nhelp(package = 'tm')\nassoc.l <- findAssocs(books.dtm, c('love','hate'), c(.46,.8))\nassoc.l[[1]]\nassoc.l[[2]]\n\n# correlation of word vectors (highest correlation)\nbooks.mat <- as.matrix(books.dtm)\ndim(books.mat)\nrownames(books.mat) <- filenames\n\nlove.v <- books.mat[,'love']\nhateth.v <- books.mat[,'hateth']\nprint(cor(love.v,hateth.v, method = 'pearson'))\n\n\nhate.v <- books.mat[,'hate']\nprint(cor(love.v,hate.v, method = 'pearson'))\n\nterms <- c('love','loveth','hate','hateth')\n# euclidean length between two vectors\neuclid.dst <- dist(t(books.mat[,terms])) # transpose matrix\n\n# cosine similarity between words to overcome effects of document length in document clustering\ninstall.packages(\"lsa\")\nlibrary(lsa)\n?cosine\ncosine(love.v,hate.v) ==  (hate.v%*%love.v)/(sqrt(sum(hate.v^2))*sqrt(sum(love.v^2)))\ncosine.mat <- cosine(books.mat[,terms])\n# cluster for visualization\nhc.dst <- hclust(as.dist(cosine.mat))# apply clustering \nplot(hc.dst)# plot dendrogram\n# or plot principal components\nplot(prcomp(cosine.mat)$x)\ntext(prcomp(cosine.mat)$x[,1],prcomp(dist.mat)$x[,2],rownames(dist.mat))",
    "created" : 1467013365260.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2353424308",
    "id" : "5E599BB6",
    "lastKnownWriteTime" : 1469612755,
    "path" : "~/courses/au_summer_university/summer_u2016/classes/tutorials/associations.R",
    "project_path" : "associations.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "type" : "r_source"
}