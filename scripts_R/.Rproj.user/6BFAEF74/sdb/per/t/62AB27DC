{
    "contents" : "# LDA\nrm(list = ls())\nwd <- 'C:/Users/KLN/some_r'\nsetwd(wd)\nsource('util_fun.R')\n\nlibrary(slam)\n\ninput.dir <- 'C:/Users/KLN/some_r/data/nt_hist'\nfiles.v <- dir(path = input.dir, pattern='.*txt')\n\n# tokenize text in directory\nmaketext <- function(files,directory){\n  text.word.l <- list()\n  for(i in 1:length(files)){\n    text.v <- scan(paste(directory, files[i], sep=\"/\"), what=\"character\", sep=\"\\n\") # read a file\n    text.v <- paste(text.v, collapse=\" \") # collapse lines\n    text.lower.v <- tolower(text.v) # casefolding\n    text.words.v <- strsplit(text.lower.v, \"\\\\W\") # tokenize\n    text.words.v <- unlist(text.words.v) # transform list to vector\n    text.words.v <- text.words.v[which(text.words.v!=\"\")] # remove blanks\n    text.word.l[[files[i]]] <- text.words.v # update list\n  }\n  names(text.word.l) <- gsub(\"\\\\..*\",\"\",files)\n  return(text.word.l)\n}  \n\n# slice text in n bins\nslice_text <- function(text,bin){\n  sliced.text.l <- split(text, cut(1:length(text),bin))\n}\n\n\ntext.word.l <- maketext(files.v,input.dir)\nnames(text.word.l) <- gsub(\"\\\\..*\",\"\",files.v)\ntext.l <- unlist(lapply(text.word.l,slice_text,2), recursive=FALSE)# why could slice size matter?\nfilenames.v <- gsub(\"\\\\..*\",\"\",names(text.l))\n\n# create corpus from slices\nlibrary(tm)\ntext.cor <- Corpus(VectorSource(lapply(text.l, paste, collapse = \" \")))\n# clean and filter\ntext.cor <- tm_map(text.cor, removeNumbers)\ntext.cor <- tm_map(text.cor, removeWords, stopwords(\"english\"))\ntext.cor <- tm_map(text.cor, stripWhitespace)\n\n## create document term matrix\ntext.dtm <- DocumentTermMatrix(text.cor)\nprint(text.dtm)\ntext.dtm <- docsparse(2,text.dtm)\nprint(text.dtm)\n\nsummary(col_sums(text.dtm))\n# prune dtm\nprune <- function(dtm,mx){\n  mx <- ceiling(dim(dtm)[1]*mx)\n  dtm <- dtm[,slam::col_sums(as.matrix(dtm) > 0) < mx]\n  return(dtm)\n}\ntext.dtm <- prune(text.dtm,.75)# try other levels of pruning\nsummary(col_sums(text.dtm))\ntext.dtm\n\n# extract Thomas\nthom.idx <- filenames.v == 'Thomas'\nthom.dtm <- text.dtm[thom.idx,]\ntext.dtm <- text.dtm[!thom.idx,]\ntext.dtm$dimnames$Docs <- filenames.v[!thom.idx]\n\n\n# train topic model based latent dirichlet allocation\nlibrary(topicmodels) # Based on Blei's code\nls('package:topicmodels')# show functions in library\nk = 20 # number of topics\nseed <- 1234\nmdl1 <- LDA(text.dtm, k = k, method = 'VEM', control = list(seed = seed))\n## unpacking the model\n\n# quick & dirty\nterms(mdl1,5)\ntopics(mdl1,2)\n\n\n\n\n####################################################################\n# proportions parameter\nalpha <- mdl1@alpha\nprint(alpha)\n# lexicon\nlexicon <- mdl1@terms\nhead(lexicon)\n\n# topics' word distribution (these estimate are only semi-meaningful)\ntopicword.mat <- mdl1@beta\ndim(topicword.mat)\nterms(mdl1,10)# print the 10 most likely terms within each topic\n\n# documents' topic distribution\ndoctopic.mat <- mdl1@gamma\ndim(doctopic.mat)\ndoctopic.mat[1,]# topic saturation of document 1\nrow_sums(doctopic.mat)[1]\n# plot document distribution\nbarplot(doctopic.mat[1,])\n\nlibrary(ggplot2)\ndev.new()\ni = 6 # document number to plot\n  dt.df <- data.frame(x = 1:length(doctopic.mat[i,]), y = doctopic.mat[i,])\n  ggplot(data = dt.df, aes(x = x, y = y)) +\n  geom_bar(stat = \"identity\", colour =\"#FF9999\")+\n  theme_minimal() +\n  scale_x_discrete('Topic', breaks = 1:k, limits = as.character(1:k)) +\n  ylab(\"Document weight\") +\n  labs(title = paste(text.dtm$dimnames$Docs[i]))\n\n\n# calculate posteriors for words within each topic\nmdl1post.l <- posterior(mdl1, text.dtm)\nstr(mdl1post.l)\n# topic 4\ntmp <- sort(mdl1post.l$terms[4,], decreasing = T)# word posteriors for each topic p(w|k)=Ï•kw\n\n# and now the obligatory topic word cloud\nlibrary(wordcloud); library(RColorBrewer)\npal2 <- brewer.pal(8,\"Dark2\")\ndev.new()\nwordcloud(names(tmp[1:20]),tmp[1:20], scale=c(8,.2), random.order=FALSE, rot.per=.15, colors=pal2)\nfilenames.v[4]\n# perplexity\n# model evaluation\nperplexity(mdl1)\nperplexity(mdl1,thom.dtm)\n\n# infer topics of unseen documents\nmdl1new.l <- posterior(mdl1, thom.dtm)\nstr(mdl1new.l)\nbarplot(mdl1new.l$topics[1,])# topic distribution for document 1 of unseen data\n\n\n### scaling\n# necessary\nlibrary(tm)\nlibrary(topicmodels)\n# make life easier\nlibrary(plyr)\nlibrary(slam)\n\n# build corpus\ndd <- \"C:/Users/KLN/some_r/data/kjv_books\";\nbooks.cor  <- Corpus(DirSource(dd, encoding = \"UTF-8\"), readerControl = list(language = \"lat\"))\nnames(books.cor) <- gsub(\"\\\\..*\",\"\",names(books.cor))# remove ending\nfilenames <- names(books.cor)\nbooks.cor <- tm_map(books.cor, PlainTextDocument)\nbooks.cor <- tm_map(books.cor, content_transformer(tolower))\nbooks.cor <- tm_map(books.cor, removePunctuation)\nbooks.cor <- tm_map(books.cor, removeNumbers)\nbooks.cor <- tm_map(books.cor, removeWords, stopwords(\"english\"))\nbooks.cor <- tm_map(books.cor, stemDocument)\nbooks.cor <- tm_map(books.cor, stripWhitespace)\n# document term matrix\nbooks.dtm <- DocumentTermMatrix(books.cor, control = list(minWordLength = 2))\ndim(books.dtm)\nbooks.dtm <- docsparse(2,books.dtm)\ndim(books.dtm)\nsummary(col_sums(books.dtm))\nbooks.dtm <- prune(books.dtm,.75,1)\nsummary(col_sums(books.dtm))\n\n### estimate number of topics (optimal k estimation)\nk = 100\n#progress.bar <- create_progress_bar(\"text\")\n#progress.bar$init(k)\nbest.mdl <- list()\n# perplex.mat <- matrix(0,k-1,2)\nfor(i in 2:k){\n  best.mdl[[i-1]] <- LDA(books.dtm, i)\n  print(paste('k =',i, sep = ' '))\n#  progress.bar$step()\n#  save(best.mdl, file = 'someLdaMdl.RData')\n}\n#save(best.mdl,file = 'estimate_k.RData')\nload('estimate_k.RData')\nperplex.mat <- as.matrix(unlist(lapply(best.mdl, perplexity)))\nplot(perplex.mat, main= 'Parameter estimation', xlab = 'k', ylab = 'Perplexity')\n\n# unpack model\n# ten most likely terms in each topic\nterms(best.mdl[[18]],10)\n\n# two dominant topics for all documents\ntop2 <- topics(best.mdl[[18]],2)\ncolnames(top2) <- filenames\nprint(top2)\n",
    "created" : 1469804803432.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3340468667",
    "id" : "62AB27DC",
    "lastKnownWriteTime" : 1470133806,
    "path" : "~/courses/au_summer_university/summer_u2016/classes/tutorials/latent_variables.R",
    "project_path" : "latent_variables.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 25,
    "source_on_save" : false,
    "type" : "r_source"
}