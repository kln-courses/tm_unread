{
    "contents" : "setwd('C:\\\\Users\\\\KLN\\\\Documents\\\\textAnalysis\\\\melville')\n\n\n##### micro analysis\n\n# loading raw data\ntext.v <- scan(\"melville.txt\", what = \"character\", sep=\"\\n\")\nhead(text.v)\nanalects.v <- scan(\"analects.txt\", what = \"character\", sep=\"\\n\",encoding = \"UTF-8\")\nhead(analects.v)\n\n# separate content from metadata\nstart.v <- which(text.v == \"CHAPTER 1. Loomings.\")\nend.v <- which(text.v == \"orphan.\")\nstart.metadata.v <- text.v[1:start.v-1]\nend.metadata.v <- text.v[(end.v+1:length(text.v))]\nmetadata.v <- c(start.metadata.v,end.metadata.v)\nhead(metadata.v)\ntail(metadata.v)\n\n# main novel\nnovel.lines.v <- text.v[start.v:end.v]\nhead(novel.lines.v)\ntail(novel.lines.v)\nnLines <- length(novel.lines.v)\n\n# collapse all lines into one string\nnovel.v <- paste(novel.lines.v, collapse=\" \")\nanalectsVect <- paste(analects.v, collapse=\" \")\n\n# Reprocessing the content\n# transform to lower case\nnovel.lower.v <- tolower(novel.v)\n\n# split string to token list\nmoby.words.l <- strsplit(novel.lower.v, \"\\\\W\") # regex: matches non-alphanumeric character\nanalectsWords <- strsplit(analectsVect,\"\\\\s\") # regex: matches whitespace\n# transform to vector\nmoby.word.v <- unlist(moby.words.l)\nanalectsVect2 <- unlist(analectsWords)# workspace can't display utf-8\n\n# remove blanks (and punctuation from analects)\nnot.blanks.v <- which(moby.word.v!=\"\")\nhead(not.blanks.v)\nmoby.word.v <- moby.word.v[not.blanks.v]\nmoby.word.v[1:10]\n# remove ？ from analects\nnotQuest <- which(analectsVect2 != \"？\")\nanalectsVect2 <- analectsVect2[notQuest]\n# comment: continue with remaining punctuation ...\n\n# identify relevant words\n\n# word frequency table\nmoby.freqs.t <- table(moby.word.v)\nhead(moby.freqs.t)\ntail(moby.freqs.t)\nanalectsFreq <- table(analectsVect2)\n\n# export frequency table to .xlsx file\ntest <- as.data.frame(analectsFreq)# convert to data frame \nlibrary(xlsx) #load xlsx package and write file\nwrite.xlsx(test, file = \"test.excelfile.xlsx\", sheetName = \"TestSheet\", row.names = FALSE)\n\n# sort word frequencies tables\nsorted.moby.freqs.t <- sort(moby.freqs.t, decreasing = TRUE)\nhead(sorted.moby.freqs.t)\nanalectsFreqSort <- sort(analectsFreq, decreasing = TRUE)\n\n# build relative word frequencies\nsorted.moby.rel.freqs.t <- 100*(sorted.moby.freqs.t/sum(sorted.moby.freqs.t))\nanalectsRelFreq <- 100*(analectsFreqSort/sum(analectsFreqSort))\n# plot relative word frequencies (test Zipf's law)\nplot(sorted.moby.rel.freqs.t[1:10], type=\"b\",\n     xlab=\"Top Ten Words\",ylab=\"Percentage of full text\", xaxt=\"n\")\naxis(1,1:10,labels=names(sorted.moby.rel.freqs.t[1:10]))\n\ntopTen = c(3,7:11,13:15,17)# have not removed all punctuation\nplot(analectsRelFreq[topTen], type=\"b\",\n     xlab=\"Top Ten Words\",ylab=\"Percentage of full text\", xaxt=\"n\")\naxis(1,1:10,labels=names(analectsRelFreq[topTen]))\n\n# 2nd order poly for modelling Zipf's law on relative frequencies\nx <- 1:10\ny <- analectsRelFreq[topTen]\nzipfMdl <- lm(y ~ poly(x, 2, raw=TRUE))\nlines(x, zipfMdl$fitted.values, col = 2) \ngoogleTransTopTen <- c(\"Son\",\"said\",\"and\",\"no\",\"can\",\"and\",\"person\",\"who\",\"to\",\"carry on\")\naxis(1,1:10,labels=googleTransTopTen)\n\n# test Zipf model\nmdl0 <- lm(y ~ 1)# null model\nmdl1 <- lm(y ~ x)# linear model\nanova(mdl0,mdl1,zipfMdl)\naic <- c(AIC(mdl0),AIC(mdl1),AIC(zipfMdl))\ntext(x=9, y = c(3,3.5,4), label=aic)\n\n# check specific word frequency\nsorted.moby.freqs.t[\"whale\"]\nsorted.moby.freqs.t[\"ahab\"]\nsorted.moby.rel.freqs.t[\"whale\"]\nsorted.moby.rel.freqs.t[\"ahab\"]\n\n### token distribution analysis\nhead(moby.word.v)\nn.time.v <- seq(1:length(moby.word.v))# t aka 'narrative time'\n# identify occurences of whale\nwhales.v <- which(moby.word.v == \"whale\")\n# build vector with 1 for whale and NA for eveything else for dispersion plot\nw.count.v <- rep(NA,length(n.time.v))\nw.count.v[whales.v] <- 1\n# plot\nplot(w.count.v, main=\"Dispersion plot of 'whale' in Moby-Dick\",\n     xlab=\"Novel time\",ylab=\"whale\",type=\"h\",ylim=c(0,1),yaxt='n')\n\n# repeat with ahab\nahabs.v <- which(moby.word.v == \"ahab\")\na.count.v <- rep(NA,length(n.time.v))\na.count.v[ahabs.v] <- 1\n\nplot(a.count.v, main=\"Dispersion plot of 'ahab' in Moby-Dick\",\n     xlab=\"Novel time\",ylab=\"ahab\",type=\"h\",ylim=c(0,1),yaxt='n')\n\n# combine the two plots\npar(mfrow=c(2,1))\nplot(w.count.v, main=\"Dispersion plot of 'whale' in Moby-Dick\",\n     xlab=\"Novel time\",ylab=\"whale\",type=\"h\",ylim=c(0,1),yaxt='n')\nplot(a.count.v, main=\"Dispersion plot of 'ahab' in Moby-Dick\",\n     xlab=\"Novel time\",ylab=\"ahab\",type=\"h\",ylim=c(0,1),yaxt='n')\n\n## Chapter structure/the novel's own internal structure\n# clean and import text\nsetwd('C:\\\\Users\\\\KLN\\\\Documents\\\\textAnalysis\\\\melville')\nrm(list = ls()) # clean workspace\ntext.v <- scan(\"melville.txt\", what = \"character\", sep=\"\\n\")# regex newline \\n\nstart.v <- which(text.v == \"CHAPTER 1. Loomings.\")\nend.v <- which(text.v == \"orphan.\")\nnovel.lines.v <- text.v[start.v:end.v]\n# grep to find chapter breaks (i.e. pattern matching)\nchap.positions.v <- grep(\"^CHAPTER \\\\d\", novel.lines.v)# match CHAPTER\nnovel.lines.v[chap.positions.v] \n# still needs a marker for the end of the last chapter, so we can fetch the content between chapter markers\nnovel.lines.v <- c(novel.lines.v,\"END\")# add end\nlast.position.v <- length(novel.lines.v)\nchap.positions.v <- c(chap.positions.v,last.position.v)\nchap.positions.v\n# fetch chapter content\nchapter.raws.l <- list()\nchapter.freqs.l <- list()\n\nfor(i in 1:length(chap.positions.v)){\n  if(i != length(chap.positions.v)){\n  chapter.title <- novel.lines.v[chap.positions.v[i]]\n  start <- chap.positions.v[i]+1\n  end <- chap.positions.v[i+1]-1\n  chapter.lines.v <- novel.lines.v[start:end]\n  chapter.words.v <- tolower(paste(chapter.lines.v, collapse=\" \"))\n  chapter.words.l <- strsplit(chapter.words.v, \"\\\\W\")\n  chapter.word.v <- unlist(chapter.words.l)\n  chapter.word.v <- chapter.word.v[which(chapter.word.v!=\"\")]\n  chapter.freqs.t <- table(chapter.word.v)\n  chapter.raws.l[[chapter.title]] <- chapter.freqs.t\n  chapter.freqs.t.rel <- 100*(chapter.freqs.t/sum(chapter.freqs.t))\n  chapter.freqs.l[[chapter.title]] <- chapter.freqs.t.rel\n  }\n}\n\n\n## accessing and porcessing list items (i.e., chapter list)\n# search specific word in chapter\nchapter.freqs.l[[1]][\"whale\"]\n# build list of whale frequency for each chapter\nwhale.l <- lapply(chapter.freqs.l,'[','whale')\n# convert list to matrix\nwhales.m <- do.call(rbind,whale.l)\n\nahab.l <- lapply(chapter.freqs.l,'[','ahab')\nahabs.m <- do.call(rbind,ahab.l)\n\n## line graph of whale and ahab in chapters  and time series analysis in matlab, see melville.m\n#whales.m[is.na(whales.m)] <- 0\n#ahabs.m[is.na(ahabs.m)] <- 0\n#x <- 1:length(whales.m)\n#plot(x,whales.m,type=\"l\",xlab = 'Chapters',ylab = 'Frequency',col=\"red\")\n#points(x,whales.m,col=\"red\")\n#points(x,ahabs.m)\n#lines(x,ahabs.m)\n## export frequency table to .xlsx file\n#whales.df <- as.data.frame(whales.m)\n#ahabs.df <- as.data.frame(ahabs.m)# convert to data frame \n#library(xlsx) #load xlsx package and write file\n#write.xlsx(whales.df, file = \"whales.xlsx\", sheetName = \"TestSheet\", row.names = FALSE)\n#write.xlsx(ahabs.df, file = \"ahabs.xlsx\", sheetName = \"TestSheet\", row.names = FALSE)\n\nwhales.v <- whales.m[,1]\nahabs.v <- ahabs.m[,1]\nwhales.ahabs.m <- cbind(whales.v,ahabs.v)\ncolnames(whales.ahabs.m) <- cbind(\"whale\",\"ahab\")\n\nbarplot(whales.ahabs.m , beside = T, col = \"grey\")\n\n### correlation\n# replace NA with 0 in matrix\nwhales.ahabs.m[which(is.na(whales.ahabs.m))]<-0\ncor(whales.ahabs.m)\ncor.test(whales.ahabs.m[,1],whales.ahabs.m[,2])\n# testing the correlation using data frames and simalation\ncor.data.df <- as.data.frame(whales.ahabs.m)\ncor(cor.data.df)\n# random whale vector\nsample(cor.data.df$whale)\ncor(sample(cor.data.df$whale),cor.data.df$ahab)\nmycors.v <- NULL\nfor(i in 1:10000){\n  mycors.v <- c(mycors.v, cor(sample(cor.data.df$whale),cor.data.df$ahab))\n}\nh <- hist(mycors.v, breaks = 100, col = 'grey',\n          xlab=\"Correlation coefficients with normal curve\", main=\"Simulation of correlation coefficients\", plot=T)\n#title(\"Simulation of correlation coefficients\")\nxfit <- seq(min(mycors.v),max(mycors.v),length=1000)\nyfit <- dnorm(xfit,mean = mean(mycors.v),sd = sd(mycors.v))\nyfit <- yfit*diff(h$mids[1:2])*length(mycors.v)\nlines(xfit,yfit,col=\"black\",lwd=2)\n\n##### meso analysis\nrm(list = ls())\nsetwd('C:\\\\Users\\\\KLN\\\\Documents\\\\textAnalysis\\\\melville')\ntext.v <- scan(\"melville.txt\", what = \"character\", sep=\"\\n\")\n### lexical variety\n# start-up code\nstart.v <- which(text.v == \"CHAPTER 1. Loomings.\")\nend.v <- which(text.v == \"orphan.\")\nnovel.lines.v <-  text.v[start.v:end.v]\nnovel.lines.v <- unlist(novel.lines.v)\nchap.positions.v <- grep(\"^CHAPTER \\\\d\", novel.lines.v)\nlast.position.v <-  length(novel.lines.v)\nchap.positions.v  <-  c(chap.positions.v , last.position.v)\nchapter.freqs.l <- list()\nchapter.raws.l <- list()\nfor(i in 1:length(chap.positions.v)){\n  if(i != length(chap.positions.v)){\n    chapter.title <- novel.lines.v[chap.positions.v[i]]\n    start <- chap.positions.v[i]+1\n    end <- chap.positions.v[i+1]-1\n    chapter.lines.v <- novel.lines.v[start:end]\n    chapter.words.v <- tolower(paste(chapter.lines.v, collapse=\" \"))\n    chapter.words.l <- strsplit(chapter.words.v, \"\\\\W\")\n    chapter.word.v <- unlist(chapter.words.l)\n    chapter.word.v <- chapter.word.v[which(chapter.word.v!=\"\")] \n    chapter.freqs.t <- table(chapter.word.v)\n    chapter.raws.l[[chapter.title]] <-  chapter.freqs.t\n    chapter.freqs.t.rel <- 100*(chapter.freqs.t/sum(chapter.freqs.t))\n    chapter.freqs.l[[chapter.title]] <- chapter.freqs.t.rel\n  }\n}\n## mean word frequency ('recycling of words')\nlength(chapter.raws.l)# how many chapters?\nnames(chapter.raws.l)# what are they called\nclass(chapter.raws.l$\"CHAPTER 1. Loomings.\")# acces class of chapter 1 in list\n# or\nclass(chapter.raws.l[[1]])\nchapter.raws.l$\"CHAPTER 1. Loomings.\"# word frequency table for chapter \n# or\nchapter.raws.l[[1]]\n# total number of words in chapter 1\nsum(chapter.raws.l[[1]])\n# unique number of words\nlength(chapter.raws.l[[1]])\n# mean word frequency\nsum(chapter.raws.l[[1]])/length(chapter.raws.l[[1]])\n# or\nmean(chapter.raws.l[[1]])\n## extracting word usage means\nlapply(chapter.raws.l,mean)\nmean.word.use.m <- do.call(rbind,lapply(chapter.raws.l,mean))\ndim(mean.word.use.m)\nplot(mean.word.use.m, type=\"h\",xlab=\"Chapter\",ylab=\"Mean frequency\", main=\"Moby-Dick\")\n# mean scaling\nplot(scale(mean.word.use.m),type=\"h\")\n# order chapters on mean frequency\nmean.word.use.m[order(mean.word.use.m, decreasing=TRUE),]\n## Type-token ratio or TTR (percentage of high to low lexical variety)\nlength(chapter.raws.l[[1]])/sum(chapter.raws.l[[1]])*100 #TTR for chapter 1\nttr.l <- lapply(chapter.raws.l,function(x){length(x)/sum(x)*100})\nttr.m <- do.call(rbind,ttr.l)\n# order ttr\nttr.m[order(ttr.m,decreasing=TRUE)]\n# plot ttr on chapter\nplot(ttr.m,type=\"h\")\n\n## PARSING XML\nrm(list = ls())\nsource(\"corpusFunctions.R\")\ninstall.packages(\"XML\")\nlibrary(\"XML\")\n# load xml file\ndoc <- xmlTreeParse(\"melville1.xml\", useInternalNodes = TRUE)\n# access documents\nchapters.ns.l <- getNodeSet(doc,\n                            \"/tei:TEI//tei:div1[@type='chapter']\",\n                            namespaces = c(tei = \"http://www.tei-c.org/ns/1.0\"))\n#chapters.ns.l[[1]]# chapter 1\n#class(chapters.ns.l[[1]])\n#length(chapters.ns.l)\n\n# generate word table from xml file\nchapter.freqs.l <- list()\nchapter.raws.l <- list()\nfor(i in 1:length(chapters.ns.l)){\n  # get chapter titles from head element\n  chap.title <- xmlValue(xmlElementsByTagName(chapters.ns.l[[i]],\"head\")[[1]])\n  # get only the contents of the paragraph tags\n  paras.ns <- xmlElementsByTagName(chapters.ns.l[[i]], \"p\")\n  # combine all words from every paragraph\n  chap.words.v <- paste(sapply(paras.ns,xmlValue), collapse=\" \")\n  # convert to lower case\n  words.lower.v <- tolower(chap.words.v)\n  # tokenize\n  words.l <- strsplit(words.lower.v, \"\\\\W\")\n  word.v <- unlist(words.l)\n  word.v <- word.v[which(word.v != \"\")]\n  # calculate frequencies\n  chapter.freqs.t <- table(word.v)\n  chapter.raws.l[[chap.title]] <- chapter.freqs.t\n  chapter.freqs.l[[chap.title]] <- 100*(chapter.freqs.t/sum(chapter.freqs.t))\n}\n\n# test on whale and ahab plot\nwhales <- do.call(rbind, lapply(chapter.freqs.l, '[', 'whale'))\nahabs <- do.call(rbind, lapply(chapter.freqs.l, '[', 'ahab'))\nwhales.ahabs <- cbind(whales,ahabs)\nwhales.ahabs[which(is.na(whales.ahabs))] <- 0\n(colnames(whales.ahabs) <- c(\"whale\",\"ahab\")) # wrap in parenthesis to display assignment in console\nbarplot(whales.ahabs, beside = T, col=\"grey\")\nwhales.ahabs.df <- as.data.frame(whales.ahabs)\ncor.test(whales.ahabs.df$whale,whales.ahabs.df$ahab)\n\n\n## CLUSTERING\nrm(list = ls())\nsetwd('C:\\\\Users\\\\KLN\\\\Documents\\\\textAnalysis\\\\melville\\\\TextAnalysisWithR')\nlibrary(XML)\ninput.dir <- \"data/XMLAuthorCorpus\"\nfiles.v <- dir(path = input.dir, pattern=\".*xml\")\n\n# function for word frequencies from XML documents\ngetTEIWordTableList <- function(doc.object){\n  paras <- getNodeSet(doc.object,\"/tei:TEI/tei:text/tei:body//tei:p\", c(tei = \"http://www.tei-c.org/ns/1.0\"))\n  words <- paste(sapply(paras,xmlValue), collapse=\" \")\n  words.lower <- tolower(words)\n  words.l <- strsplit(words.lower, \"\\\\W\")\n  word.v <- unlist(words.l)\n  books.freqs.t <- table(word.v[which(word.v!=\"\")])\n  book.freqs.rel.t <- 100*(books.freqs.t/sum(books.freqs.t))\n  return(book.freqs.rel.t)\n}\n\n# build word frequency list for all the xml documents using \nbook.freqs.l <- list()\nfor(i in 1:length(files.v)){\n  doc.object <- xmlTreeParse(file.path(input.dir,files.v[i]), useInternalNodes = TRUE)\n  worddata <- getTEIWordTableList(doc.object)\n  book.freqs.l[[files.v[i]]] <- worddata\n}\n\nclass(book.freqs.l)\nnames(book.freqs.l)\nstr(book.freqs.l)\n\n# convert list to data frame\nfreqs.l <- mapply(data.frame, ID=seq_along(book.freqs.l), book.freqs.l, SIMPLIFY=FALSE, MoreArgs=list(stringsAsFactors=FALSE))\nclass(freqs.l[[1]])\nclass(book.freqs.l[[1]])\n\n# bind to single three-column data frame (long format)\nfreqs.df <- do.call(rbind,freqs.l)\ntail(freqs.df)\n\n# reshape to wide format\nresult <- xtabs(Freq ~ ID+Var1, data = freqs.df)\ndim(result)\ncolnames(result)\n# convert to matrix object\nfinal.m <- apply(result, 2, as.numeric)# 2 reference to columns, 1 reference to rows\n# winnow data to the stylistic elements/highly frequent elements\nsmaller.m <- final.m[,apply(final.m, 2, mean) >= .25]\ndim(smaller.m)\n\n# cluster\n# create distance object\ndm <- dist(smaller.m)\n# perform cluster analysis on dm\ncluster <- hclust(dm)\n# get book file names to use as labels\ncluster$labels <- names(book.freqs.l)\n# plot dendrogram\nplot(cluster)\n\n### Classification\nrm(list = ls())\nsetwd('C:\\\\Users\\\\KLN\\\\Documents\\\\textAnalysis\\\\melville\\\\TextAnalysisWithR')\nlibrary(XML)\ninput.dir <- \"data/XMLAuthorCorpus\"\nfiles.v <- dir(path = input.dir, pattern=\".*xml\")\n\n# function that removes blanks\nremoveBlanks <- function(x){\n  x[which(x!=\"\")]\n}\n\n\n# function for generating a list of word vectors, where each document is chunked according to chunk.size\ngetTEIWordSegmentTableList <- function(doc.object, chunk.size = 10){\n  paras <- getNodeSet(doc.object, \"/d:TEI/d:text/d:body//d:p\", c(d = \"http://www.tei-c.org/ns/1.0\"))\n  words <- paste(sapply(paras,xmlValue), collapse=\" \")\n  words.lower <- tolower(words)\n  words.list <- strsplit(words.lower, \"\\\\W\")\n  word.v <- unlist(words.list)\n  # chunk text\n  max.length <- length(word.v)/chunk.size # max length of chunk\n  x <- seq_along(word.v)# sequence for entire text\n  chunks.l <- split(word.v, ceiling(x/max.length))\n  chunks.l <- lapply(chunks.l, removeBlanks)# make sure to call removeBlanks\n  freq.chunks.l <- lapply(chunks.l,table)# raw frequencies\n  rel.freq.chunks.l <- lapply(freq.chunks.l, prop.table)# get relative frequencies with prop.table\n  return(rel.freq.chunks.l)\n}\n\n# generate relative word frequencies for each of the texts each in 10 chunks ~ a list of a list of tables\nbook.freqs.l <- list()\nfor(i in 1:length(files.v)){\n  doc.object <- xmlTreeParse(file.path(input.dir,files.v[i]), useInternalNodes=TRUE)\n  chunk.data.l <- getTEIWordSegmentTableList(doc.object, 10)\n  book.freqs.l[[files.v[i]]] <- chunk.data.l\n}\n# explore list\nnames(book.freqs.l)\nclass(book.freqs.l)\nclass(book.freqs.l[[1]])\nclass(book.freqs.l[[1]][1])\nlength(book.freqs.l)\nlength(book.freqs.l[[1]])\nlength(book.freqs.l[[1]][1])\nstr(book.freqs.l)\nstr(book.freqs.l[[37]])# numerical index\nstr(book.freqs.l$Norris1.xml)# name\nbook.freqs.l$Norris1.xml[[1]][1:3] # specific words in chunk of text\n\n## converting list to matrix\n# function for calling mapply and rbind\nmy.mapply <- function(x){\n  my.list <- mapply(data.frame, ID=seq_along(x),x,SIMPLIFY=FALSE,MoreArgs=list(stringsAsFactors=FALSE))\n  my.df <- do.call(rbind,my.list)\n  return(my.df)\n}\n# run my.mapply iteratively on the primary list\nfreqs.l <- lapply(book.freqs.l,my.mapply)\nfreqs.df <- do.call(rbind,freqs.l)\n\nhead(freqs.df)\ntail(freqs.df)\ndim(freqs.df)\nnames(freqs.df)\n\n## organizing the data\n# create unique book IDs for every chunk\nbookids.v <- gsub(\"\\\\..*\",\"\",rownames(freqs.df))# make text based classes for each chunk based on freqs.df/  .xml.. with nothing \"\" \nbook.chunk.ids <- paste(bookids.v,freqs.df$ID, sep=\"_\") # paste new and old ID\nfreqs.df$ID <- book.chunk.ids# replace IDs\nhead(freqs.df)\ntail(freqs.df)\n\n# cross tabulation/transform from long to wide format\nresult.t <- xtabs(Freq ~ ID+Var1, data=freqs.df)\ndim(result.t)\n# convert to data frame\nfinal.df <- as.data.frame.matrix(result.t)\n\n# search for specific word type in specific documents\nfinal.df[1:10, c(\"of\",\"the\")] # of and the in the 10 chunks of the first text\n\n## mapping data to metadata\n# label variable from authorname\nmetacols.m <- do.call(rbind, strsplit(rownames(final.df),\"_\"))# split row names\nhead(metacols.m)\ncolnames(metacols.m) <- c(\"sampletext\",\"samplechunk\")# change column names\nhead(metacols.m)\nunique(metacols.m[,\"sampletext\"])# check all text names\nauthor.v <- gsub(\"\\\\d+$\",\"\",metacols.m[,\"sampletext\"])# replace digits followed by end (d+$) with nothing, to only leave author names\nunique(author.v)# check authors\nauthorship.df <- cbind(author.v,metacols.m,final.df)\ndim(authorship.df)\n## reducing the feature set using threshold of column means\nfreq.means.v <- colMeans(authorship.df[,4:ncol(authorship.df)])\nthreshold <- .005\nkeepers.v <- which(freq.means.v >= threshold)# threshold\nkeepers.v # index for stylistic features that meet the threshold\nsmaller.df <- authorship.df[, names(keepers.v)] # extract feature set\nsmaller.df <- cbind(author.v, metacols.m, smaller.df)# add labels\n  # or simply\n  smaller.df <- authorship.df[, c(names(authorship.df)[1:3], names(keepers.v))]\ndim(smaller.df)\n\n## classification with support vector machines\n# identify text to be classified (testing)\nanon.v <- which(smaller.df$author.v == \"anonymous\")\n# training set\ntrain <- smaller.df[-anon.v,4:ncol(smaller.df)]\n# class, factor\nclass.f <- smaller.df[-anon.v,\"author.v\"]\n# load package\nlibrary(e1071)\n# build model\nmodel.svm <- svm(train,class.f)\nsummary(model.svm)\n# test accuracy\npred.svm <- predict(model.svm,train)\nas.data.frame(pred.svm)\n# confusion matrix\nconfMat.t <- table(pred.svm,class.f)\naccuracy.l <- classAgreement(confMat.t)# accuracy rates\n\n# data to be classfied\ntestdata <- smaller.df[anon.v,4:ncol(smaller.df)]\n# classify/test on new data\nfinal.result <- predict(model.svm, testdata)\nas.data.frame(final.result)\n# for measures of accuracy and precision, see: http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Classification/SVM\nplot(model.svm)\n\n### topic modeling\nrm(list = ls())\nsetwd('C:\\\\Users\\\\KLN\\\\Documents\\\\textAnalysis\\\\melville\\\\TextAnalysisWithR')\nlibrary(XML)\ninputDir <- \"data/XMLAuthorCorpus\"\nfiles.v <- dir(path = inputDir, pattern=\".*xml\")\n\n## preprocessing\n# parse texts into equally sized chunks based on number of words\nchunk.size <- 1000 # number of words pr. chunk\n# chunking function for percentage or number of words\nmakeFlexTextChunks <- function(doc.object, chunk.size=1000, percentage=TRUE){\n  paras <- getNodeSet(doc.object, \"/d:TEI/d:text/d:body//d:p\", c(d = \"http://www.tei-c.org/ns/1.0\"))\n  words <- paste(sapply(paras,xmlValue), collapse=\" \")\n  words.lower <- tolower(words)\n  words.lower <- gsub(\"[^[:alnum:][:space:]']\", \" \", words.lower)\n  words.l <- strsplit(words.lower,\"\\\\s+\")\n  words.v <- unlist(words.l)\n  x <- seq_along(words.v)\n  if(percentage){\n    max.length <- length(words.v)/chunk.size\n    chunks.l <- split(words.v, ceiling(x/max.length))\n  } else {\n    chunks.l <- split(words.v, ceiling(x/chunk.size))\n    # deal with small chunks in the end\n    if(length(chunks.l[[length(chunks.l)]]) <= length(chunks.l[[length(chunks.l)]])/2){\n      chunks.l[[length(chunks.l)-1]] <- c(chunks.l[[length(chunks.l)-1]],chunks.l[[length(chunks.l)]])\n      chunks.l[[length(chunks.l)]] <- NULL\n    }\n  }\n  chunks.l <- lapply(chunks.l, paste, collapse=\" \")\n  chunks.df <- do.call(rbind, chunks.l)\n  return(chunks.df)\n}\n\ntopic.m <- NULL\nfor(i in 1:length(files.v)){\n  doc.object <- xmlTreeParse(file.path(inputDir, files.v[i]), useInternalNodes=TRUE)\n  chunk.m <- makeFlexTextChunks(doc.object,chunk.size,percentage=FALSE)\n  textname <- gsub(\"\\\\..*\",\"\",files.v[i])\n  segments.m <- cbind(paste(textname,segment=1:nrow(chunk.m), sep=\"_\"), chunk.m)\n  topic.m <- rbind(topic.m,segments.m)\n}\n# explore\nhead(topic.m)\ntopic.m[1,1]# chunks name/file segment identifier\ntopic.m[1,2]# chunk content/string of text for each segment\nstr(topic.m)\n# convert data frame and rename column headers for Mallet\ndocuments <- as.data.frame(topic.m, stringsAsFactors=F)\ncolnames(documents) <- c(\"id\",\"text\")\n\n\n# load mallet package\n# install.packages(\"mallet\")\nlibrary(mallet)\n\n# build Java object for Mallet\nmallet.instances <- mallet.import(documents$id, documents$text, \"data/stoplist.csv\", FALSE, token.regexp=\"[\\\\p{L}']+\") # default ([\\\\p{L}]+)\n# create topic model trainer object\ntopic.model <- MalletLDA(num.topics = 43)\nclass(topic.model)\n# fill topic model with textual data\ntopic.model$loadDocuments(mallet.instances)\n# access entire vocabulary\nvocabulary <- topic.model$getVocabulary()\nclass(vocabulary)\nhead(vocabulary)\nvocabulary[1:50]\n# access word frequencies from mallet\nword.freqs <- mallet.word.freqs(topic.model)\nhead(word.freqs)\n\n# hyperparameter optimization: optimization interval and burn-in\ntopic.model$setAlphaOptimization(40, 80)\n# run model with 400 iterations\ntopic.model$train(400)\n\n# unpacking the model\n\n# matrix with topics in rows and word type in rows\ntopic.words.m <- mallet.topic.words(topic.model, smoothed = TRUE, normalized = TRUE)\ndim(topic.words.m)\n# explore matix\ntopic.words.m[1:3, 1:3]\n# add column names (word types) to matrix\nvocabulary <- topic.model$getVocabulary()\ncolnames(topic.words.m) <- vocabulary\ntopic.words.m[1:3, 1:3]\n# identify first word in topic 1\ni <- match(max(topic.words.m[1,]),topic.words.m[1,])\ntopic.words.m[1, i]\n# compare keywords\nkeywords <- c(\"california\",\"ireland\")\ntopic.words.m[,keywords]\n# calculate which topics/rows has the highest concentration of these key terms\nimp.row <- which(rowSums(topic.words.m[,keywords]) == max(rowSums(topic.words.m[,keywords])))\n\n\n# exploring most heavily weighted words in topics\n# topic 18, 10 top words\nmallet.top.words(topic.model,topic.words.m[18,],10)\n\n# word cloud representations\n# install.packages(\"wordcloud\")\nlibrary(wordcloud)\n# grab 100 top words from topic 18\ntopic.top.words <- mallet.top.words(topic.model,topic.words.m[1,],50)\nwordcloud(topic.top.words$words,topic.top.words$weights,c(4,.5),rot.per=0,random.order=F)\n\n# access the proportion of a document that is about each topic\ndoc.topics.m <- mallet.doc.topics(topic.model,smoothed=T,normalized=T)\n\n# explore mean topical calues across document segments\nfile.ids.v <- documents[,1]# file id variable\nhead(file.ids.v)\n# split identifier\nfile.id.l <- strsplit(file.ids.v,\"_\")\nfile.chunk.id.l <- lapply(file.id.l,rbind)\nfile.chunk.id.m <- do.call(rbind,file.chunk.id.l)\nhead(file.chunk.id.m)\ndoc.topics.df <- as.data.frame(doc.topics.m) # convert to dataframe \ndoc.topics.df <- cbind(file.chunk.id.m[,1],doc.topics.df) # combine id and topic weights\nhead(doc.topics.df)\n# build data frame with mean topic weight for each document (not segment)\ndoc.topics.means.df <- aggregate(doc.topics.df[,2:ncol(doc.topics.df)],list(doc.topics.df[,1]),mean)\nhead(doc.topics.means.df)\n# visualize topic weight for each document\nbarplot(doc.topics.means.df[,\"V6\"],names.arg=c(1:43))\nplot(1:43,doc.topics.means.df[,\"V6\"],xlab=\"Documents\",ylab=\"Mean weights on Topic 6\")\n# get file name for specific document (of document 25 from Group.1 variable in means matrix)\nfilename <- as.character(doc.topics.means.df[25, \"Group.1\"])\n\n\n## pre-processing with POS tagger\n\n#install.packages(\"openNLP\")\n#install.packages(\"NLP\")\n\nlibrary(openNLP)\nlibrary(NLP)\n\n\n# code for pos tagging XML documents\nfor(i in 1:length(files.v)){\n  doc.object <- xmlTreeParse(file.path(inputDir, files.v[i]), useInternalNodes=TRUE)\n  paras <- getNodeSet(doc.object, \"/d:TEI/d:text/d:body//d:p\", c(d = \"http://www.tei-c.org/ns/1.0\"))\n  words <- as.String(paste(sapply(paras,xmlValue),collapse=\" \"))\n  # need sentence and word tokens first\n  sent_token_annotator <- Maxent_Sent_Token_Annotator()\n  word_token_annotator <- Maxent_Word_Token_Annotator()\n  a2 <- annotate(words,list(sent_token_annotator,word_token_annotator))\n  # add pos tags\n  pos_tag_annotator <- Maxent_POS_Tag_Annotator()\n  a3 <- annotate(words, pos_tag_annotator, a2)\n  a3w <- subset(a3, type == \"word\")\n  tags <- sapply(a3w$features, `[[`, \"POS\")\n  tagged_text <- paste(sprintf(\"%s/%s\", words[a3w], tags), collapse=\" \")\n  write(tagged_text, paste(\"data/taggedCorpus/\", files.v[i],\".txt\", sep=\"\"))\n}\n\n# work with POS tagged txt documents\nrm(list = ls())\nsetwd('C:\\\\Users\\\\KLN\\\\Documents\\\\textAnalysis\\\\melville\\\\TextAnalysisWithR')\nlibrary(XML)\ninputDir <- \"data/taggedCorpus\"\nfiles.v <- dir(path=inputDir,pattern=\".*xml\")\n## chunk text only with nouns (shorter chunks)\nchunk.size <- 500\n# function for splitting tagged file into vector with single word/POS pairs\nsplitText <- function(text){\n  unlist(strsplit(text,\" \"))\n}\n# function for selecting target POS marker\nselectTaggedWords <- function(tagged.words,target.tag){\n  tagged.words[grep(target.tag,tagged.words)]\n}\n# function to remove POS tag\nremoveTags <- function(word.pos){\n  sub(\"/[A-Z]{2,3}\",\"\",word.pos)\n}\n# new POS tag version of make flexible chunks\nmakeFlexChunksFromTagged <- function(tagged.text,chunk.size=500,percentage=TRUE){\n  tagged.words <- splitText(tagged.text)\n  tagged.words.keep <- c(selectTaggedWords(tagged.words,\"/NN$\"))\n  words <- removeTags(tagged.words.keep)\n  words.lower <- tolower(words)\n  words.v <- gsub(\"[^[:alnum:][:space:]']\", \" \", words.lower)\n  x <- seq_along(words.v)\n  if(percentage){\n    max.length <- length(words.v)/chunk.size\n    chunks.l <- split(words.v, ceiling(x/max.length))\n  } else {\n    chunks.l <- split(words.v, ceiling(x/chunk.size))\n    if(length(chunks.l[[length(chunks.l)]]) <= length(chunks.l[[length(chunks.l)]])/2){\n      chunks.l[[length(chunks.l)-1]] <- c(chunks.l[[length(chunks.l)-1]],chunks.l[[length(chunks.l)]])\n      chunks.l[[length(chunks.l)]] <- NULL\n    }\n  }\n  chunks.l <- lapply(chunks.l, paste, collapse=\" \")\n  chunks.df <- do.call(rbind, chunks.l)\n  return(chunks.df)\n}\n# run makeFlexChunksFromTagged on all the files --> build chunk matrix\n\ntopic.m <- NULL\nfor(i in 1:length(files.v)){\n  tagged.text <- scan(file.path(inputDir,files.v[i]), what=\"character\", sep=\"\\n\")\n  chunk.m <- makeFlexChunksFromTagged(tagged.text,chunk.size,percentage=FALSE)\n  textname <- gsub(\"\\\\..*\",\"\",files.v[i])\n  segments.m <- cbind(paste(textname,segment=1:nrow(chunk.m),sep=\"_\"),chunk.m)\n  topic.m <- rbind(topic.m, segments.m)\n}\nhead(topic.m)\nstr(topic.m)\ntopic.m[1,2]\n\n# build topic model\nlibrary(mallet)\ndocuments <- as.data.frame(topic.m, stringsAsFactors=F)# always check 'stringsAsFactors=F' \ncolnames(documents) <- c(\"id\",\"text\")\nstr(documents)\n\nmallet.instances <- mallet.import(documents$id,documents$text,\"data/stoplist.csv\",FALSE,token.regexp=\"[\\\\p{L}']+\")\ntopic.model <- MalletLDA(num.topics=43)\ntopic.model$loadDocuments(mallet.instances)\nvocabulary <- topic.model$getVocabulary()\nword.freqs <- mallet.word.freqs(topic.model)\ntopic.model$train(400)\ntopic.words.m <- mallet.topic.words(topic.model,smoothed=TRUE,normalized=TRUE)\ncolnames(topic.words.m) <- vocabulary\n\nlibrary(wordcloud)\ntopic.top.words <- mallet.top.words(topic.model,topic.words.m[37,],50)\npar(mfrow=c(1,2))\nwordcloud(topic.top.words$words,topic.top.words$weights,c(4,.5),rot.per=0,random.order=F)\nplot(topic.top.words$weights, xlab = 'Word index', ylab='Topic weight')\n\n\n",
    "created" : 1468312983544.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "26477682",
    "id" : "36CBFDC7",
    "lastKnownWriteTime" : 1413795326,
    "path" : "~/textAnalysis/melville/melville1.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 11,
    "source_on_save" : false,
    "type" : "r_source"
}