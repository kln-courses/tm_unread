{
    "contents" : "# classification\nrm(list = ls())\nwd <- 'C:/Users/KLN/some_r'\nsetwd(wd)\nsource('util_fun.R')\n\ninput.dir <- 'C:/Users/KLN/some_r/data/nt_hist'\nfiles.v <- dir(path = input.dir, pattern='.*txt')\n\n# tokenize text in directory\nmaketext <- function(files,directory){\n  text.word.l <- list() # set up empty list\n  for(i in 1:length(files)){ # loop over the files in directory\n    text.v <- scan(paste(directory, files[i], sep=\"/\"), what=\"character\", sep=\"\\n\") # read a file\n    text.v <- paste(text.v, collapse=\" \") # collapse lines\n    text.lower.v <- tolower(text.v) # casefolding\n    text.words.v <- strsplit(text.lower.v, \"\\\\W\") # tokenize\n    text.words.v <- unlist(text.words.v) # transform list to vector\n    text.words.v <- text.words.v[which(text.words.v!=\"\")] # remove blanks\n    text.word.l[[files[i]]] <- text.words.v # update list\n  }\n  names(text.word.l) <- gsub(\"\\\\..*\",\"\",files)\n  return(text.word.l)\n}  \n\ntext.word.l <- maketext(files.v,input.dir)\nnames(text.word.l)\ntexttitle.l <- gsub(\"\\\\..*\",\"\",files.v)\n\n# slice tokenized text in n chuncks\nnchunk <- function(tokens,n){\n  maxlen <- length(tokens)/n\n  x <- seq_along(tokens)\n  chunks.l <- split(tokens, ceiling(x/maxlen))\n}\n\nnslice.l <- lapply(text.word.l,nchunk,100)\n# unlist multiple list while preserving sublist names\ntext.l <- unlist(nslice.l, recursive=FALSE)\n\n# get class information\nclass.v <- gsub('\\\\..*','',names(text.l))\n# create corpus from chunks\nlibrary(tm)\ntext.l <- lapply(text.l, paste, collapse = \" \")\ntext.vs <- VectorSource(text.l)\ntext.cor <- Corpus(text.vs)\n# clean and filter\ntext.cor <- tm_map(text.cor, removeNumbers)\ntext.cor <- tm_map(text.cor, removeWords, stopwords(\"english\"))\ntext.cor <- tm_map(text.cor, stemDocument)\ntext.cor <- tm_map(text.cor, stripWhitespace)\n\n## create document term matrix\ntext.dtm <- DocumentTermMatrix(text.cor)\ntext.dtm <- docsparse(25,text.dtm)\nprint(text.dtm)\n# transform to matrix object\ntext.mat <- as.matrix(text.dtm)\nrownames(text.mat) <- names(text.l)\ntext.mat[1:10,1:10]\n\n\n# full model predict class of Thomas and build data frame\nidx <- which(class.v != 'Thomas')\n\n# classical validation procedure\ntrain <- ifelse(runif(length(class.v)) < 0.80,1,0)\nidx <- which(train == 1)\n\n# build feature set for training\nfeat1.df <- data.frame(book = class.v[idx],text.mat[idx,])\nhead(feat1.df[1:10,1:10])\n# naive bayes classifier (categorical data, but assumes independence)\nlibrary(e1071)\nmodel.nb <- naiveBayes(book ~ ., data = feat1.df)\npred.v <- predict(model.nb, feat1.df)\n\n# conditional posterior probabilities\npredraw.v <- predict(model.nb, feat1.df,type = 'raw')\nhead(predraw.v)\n\n\nconfusion.mat <- as.matrix(table(pred = pred.v,true = feat1.df$book))\n\n# performance metric\naccuracy <- sum(diag(confusion.mat))/sum(confusion.mat)\nprint(accuracy)\n\n# plot confusion\nlibrary(ggplot2)\ndev.new()\nplot <- ggplot(as.data.frame(confusion.mat))\nplot + geom_tile(aes(x=pred, y=true, fill=Freq)) + \n  scale_x_discrete(name=\"Predicted Class\") + \n  scale_y_discrete(name=\"True Class\") + \n  scale_fill_gradient(breaks=seq(from=-.5, to=4, by=.2)) + \n  labs(fill=\"Frequency\")\n\n\n# chance level\ntable(feat1.df$book)\n#chance <- 100/sum(table(feat1.df$book))\n\n# testing procedure\nidx <- which(train != 1)\nfeat2.df <- data.frame(book = class.v[idx],text.mat[idx,]) \ntest.v <- predict(model.nb, feat2.df)\nconfusion.mat <- as.matrix(table(test.v,feat2.df$book))\naccuracy <- sum(diag(confusion.mat))/sum(confusion.mat)\n\n# predict class of Thomas (early or late)\nidx <- which(class.v == 'Thomas')\nfeat2.df <- data.frame(book = class.v[idx],text.mat[idx,]) \npredthom.v <- predict(model.nb, feat2.df)\ndev.new()\nplot(table(predthom.v))\npredAct <- data.frame(pred.v,feat1.df$book)\n\n# random performance (compare to full model)\nclassrand.v <- sample(class.v)\nfeatrand.df <- data.frame(book = classrand.v[idx],text.mat[idx,])\nmodelrand.nb <- naiveBayes(book ~ ., data = featrand.df)\npredrand.v <- predict(modelrand.nb, featrand.df)\n# conditional posterior probabilities\nconfusion.mat <- as.matrix(table(predrand.v,featrand.df$book))\n# performance metric\naccuracy <- sum(diag(confusion.mat))/sum(confusion.mat)\nprint(accuracy)\n\n\n##############################\n#scaling with RTextTools and tm\nlibrary(RWeka)\ntext.dtm <- DocumentTermMatrix(text.cor, control=list(tokenize = NGramTokenizer))\ntext.dtm <- docsparse(5,text.dtm)\nlibrary(RTextTools)\n## separate training and testing set and create a container\n# random sample for testing  data from data set\ntrainidx.v <- 1:nrow(text.dtm)\ntestidx.v <- sort(sample(trainidx.v, nrow(text.dtm)*.1, replace = FALSE, prob = NULL))\ntrainidx.v <- sort(trainidx.v[! trainidx.v%in%testidx.v])\n\n# change object type, create_analytics() only handles numeric\nclassnum.v <- as.numeric(as.factor(class.v))\n  # to transform back to original\n  factor(classnum.v, labels = unique(class.v))\n# create container\ncontainer <- create_container(text.dtm, classnum.v, trainSize=trainidx.v,\n                              testSize=testidx.v, virgin=FALSE)\n# training models\nmdl1.l <- train_models(container, algorithms='SVM')\nmdl2.l <- train_models(container, algorithms = c('SVM','NNET','TREE') )\n\n# Classifying data\nres.df <- classify_models(container, mdl2.l)\nhead(res.df)\nconfusion.mat <- as.matrix(table(res.df$SVM_LABEL, container@testing_codes))\nrownames(confusion.mat) <- colnames(confusion.mat) <- unique(class.v)\nprint(confusion.mat)\naccuracy <- sum(diag(confusion.mat))/sum(confusion.mat)\n\n# performance metrics\nanalytics <- create_analytics(container, res.df)\nclass(analytics)\nsummary(analytics)\n",
    "created" : 1468312817228.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2053849162",
    "id" : "BC936AB0",
    "lastKnownWriteTime" : 1469792045,
    "path" : "~/courses/au_summer_university/summer_u2016/classes/tutorials/classification.R",
    "project_path" : "classification.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 24,
    "source_on_save" : false,
    "type" : "r_source"
}