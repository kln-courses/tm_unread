{
    "contents" : "##### code mess from classification.R\n\n### Scaling and estimation of model accuracy with caret\n\nidx <- which(class.v != 'Thomas')\nfeature.df <- data.frame(class = as.factor(class.v[idx]), text.mat[idx,])\ntail(feature.df)\nlibrary(caret)\nlibrary(klaR)# functions for classification and visualization\nsplit <- .8\ntrain_idx <- createDataPartition(feature.df$class, p=split, list=FALSE)\ntrain.df <- feature.df[train_idx,] \ntest.df <- feature.df[-train_idx,]\nmdl.nb <- NaiveBayes(class~., data=train.df)# explain error\n# some variables that are all zero for a certain class\n# find zero variance terms\nc <- unique(feature.df$class)\nidx.l <- list()\nfor(i in 1:length(c)){\n  tmp <- feature.df[feature.df$class == c[i],]\n  idx <- which(colSums(tmp[,2:ncol(tmp)]) == 0)\n  idx.l[[i]] <- idx\n}\nnullvar.v <- as.numeric(sort(unique(unlist(idx.l))))\nfeature.df[,nullvar.v+1] <- NULL# add one to ignore class column\nsplit <- 0\ntrain_idx <- createDataPartition(feature.df$class, p=split, list=FALSE)\ntrain.df <- feature.df[train_idx,] \ntest.df <- feature.df[-train_idx,]\nmdl.nb <- NaiveBayes(class~., data=train.df)# explain error\n\n\nlibrary(caret)\nlibrary(klaR)# functions for classification and visualization\n\n# 80/20 train/test split\ndata(iris)\nsplit=0.80\ntrainIndex <- createDataPartition(iris$Species, p=split, list=FALSE)\ndata_train <- iris[ trainIndex,]\ndata_test <- iris[-trainIndex,]\nmodel <- NaiveBayes(Species~., data=data_train)\n##############################\n\n\n###############################\nSVM <- train_model(container,\"SVM\")\nGLMNET <- train_model(container,\"GLMNET\")\nMAXENT <- train_model(container,\"MAXENT\")\nSLDA <- train_model(container,\"SLDA\")\nBOOSTING <- train_model(container,\"BOOSTING\")\nBAGGING <- train_model(container,\"BAGGING\")\nRF <- train_model(container,\"RF\")\nNNET <- train_model(container,\"NNET\")\nTREE <- train_model(container,\"TREE\")\n\n# Classifying data\nresults <- classify_models(container, models)\nhead(results)\n\n\nSVM_CLASSIFY <- classify_model(container, SVM)\n\nhead(SVM_CLASSIFY,50)\nconfusion.mat <- as.matrix(table(SVM_CLASSIFY$SVM_LABEL, container@testing_codes))\nrownames(confusion.mat) <- colnames(confusion.mat) <- unique(class.v)\nprint(confusion.mat)\nacc <- sum(diag(confusion.mat))/sum(confusion.mat)\n\nGLMNET_CLASSIFY <- classify_model(container, GLMNET)\nMAXENT_CLASSIFY <- classify_model(container, MAXENT)\nSLDA_CLASSIFY <- classify_model(container, SLDA)\nBOOSTING_CLASSIFY <- classify_model(container, BOOSTING)\nBAGGING_CLASSIFY <- classify_model(container, BAGGING)\nRF_CLASSIFY <- classify_model(container, RF)\nNNET_CLASSIFY <- classify_model(container, NNET)\nTREE_CLASSIFY <- classify_model(container, TREE)\n\n# Analytics\nanalytics <- create_analytics(container, results)\nsummary(analytics)\n###########\nlibrary(RTextTools)\ndata(NYTimes)\ndata <- NYTimes[sample(1:3100,size=100,replace=FALSE),]\nmatrix <- create_matrix(cbind(data[\"Title\"],data[\"Subject\"]), language=\"english\", \n                        removeNumbers=TRUE, stemWords=FALSE, weighting=tm::weightTfIdf)\ncontainer <- create_container(matrix,data$Topic.Code,trainSize=1:75, testSize=76:100, \n                              virgin=FALSE)\nmodels <- train_models(container, algorithms=c(\"MAXENT\",\"SVM\"))\nresults <- classify_models(container, models)\nanalytics <- create_analytics(container, results)\n\nsummary(analytics)\n\n\n\n###########################\n# remove blanks\nremove_blanks <- function(x){\n  x[which(x!=\"\")]\n}\n# compute word frequencies from list of tokenized texts\nwordfreq.l <- list()\nfor(i in 1:length(text.word.l)){\n  text.word.l[[i]] <- lapply(text.word.l[[i]], remove_blanks)\n  wordfreq.l[[i]] <- table(text.word.l[[i]])\n}\n# converting list to data frame\ntext2df <- function(wordfreqs){\n  wordfreqs.l <- mapply(data.frame, ID=seq_along(wordfreqs),wordfreqs,SIMPLIFY=FALSE,MoreArgs=list(stringsAsFactors=FALSE))# transform multiple list to dataframe\n  wordfreqs.df <- do.call(rbind,wordfreqs.l)# bind rows of list into a continous data frame\n  return(wordfreqs.df)\n}\n\n\n\n\n\n###################\n# unlist multiple list while preserving sublist names\ntext.l <- unlist(slice10.l, recursive=FALSE)\n##########################\n\n###\ninput.dir <- 'C:/Users/KLN/some_r/data/nt_hist'\nfiles.v <- dir(path = input.dir, pattern='.*txt')\n\nimportfolder <- function(directory){\n  files <- dir(path = directory, pattern='.*txt')\n  text.l <- list()\n  for(i in 1:length(files)){\n    text.l[[i]] <- scan(paste(directory, files[i], sep=\"/\"), what=\"character\", sep=\"\\n\") # read a file\n  }\n  names(text.l) <- gsub(\"\\\\..*\",\"\",files)\n  return(text.l)\n}\ntext.l <- importfolder(input.dir)\nnames(text.l)\ntext.l[[1]]\n\nlibrary(tm)\ntext.cor <- as.VCorpus(text.l)",
    "created" : 1468573236508.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2517009756",
    "id" : "B8BD2858",
    "lastKnownWriteTime" : 1468573526,
    "path" : "~/courses/au_summer_university/summer_u2016/classes/classification/classification2.R",
    "project_path" : null,
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 12,
    "source_on_save" : false,
    "type" : "r_source"
}